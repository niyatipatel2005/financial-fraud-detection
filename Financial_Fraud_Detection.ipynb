{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa13814-00aa-4e4c-8990-f8cb83867418",
   "metadata": {},
   "source": [
    "**PySpark** is the Python API for Apache Spark.\n",
    "\n",
    "- Apache Spark is a powerful open-source engine for big data processing, real-time stream processing, and machine learning at scale.\n",
    "\n",
    "- PySpark lets you use Python to work with Sparkâ€™s capabilities, so you can write Python code to run fast parallel processing jobs across large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5550e13-3734-45f8-9b3b-f0727d71bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c6193-21a3-4d60-9a34-34c06fcae16a",
   "metadata": {},
   "source": [
    "##### from pyspark.sql import SparkSession\n",
    "This means:\n",
    "\n",
    "\"Import\" the SparkSession class from the pyspark.sql module.\n",
    "\n",
    "- SparkSession is the main entry point to work with DataFrames in PySpark.\n",
    "\n",
    "- Think of SparkSession as the control center where you start, configure, and run Spark commands in Python.\n",
    "\n",
    "- Import the SparkSession tool. Then, set up a Spark engine named â€˜Financial Fraud Detectionâ€™. If it's already running, use that. Otherwise, create a new one.\n",
    "- \n",
    "- Without SparkSession, you can't use PySpark features like read.csv(), DataFrame, SQL, or MLlib models.\n",
    "\n",
    "It's the first thing you write in any PySpark program â€” like import pandas in pandas-based code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf22923-7675-4c0d-b81f-904595978d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Financial Fraud Detection\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd931ed7-1f3f-4b2e-869f-089ea9910a39",
   "metadata": {},
   "source": [
    "ðŸ”¹ **header=True**\n",
    "This tells Spark: â€œThe first row in the file contains the column names.â€\n",
    "\n",
    "Without this, Spark would treat them as regular data.\n",
    "\n",
    "ðŸ”¹ **inferSchema=True**\n",
    "This tells Spark: â€œTry to guess the correct data types for each column (like int, float, string).â€\n",
    "\n",
    "Without this, Spark would treat all columns as strings, which is not what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497c76c2-23de-46ee-89a2-8017ccb7e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"fraudTrain.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc99a6a-1a50-4dac-9df3-913d11a91b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c98124a-44b6-415d-a8df-7284f22d0739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b0d3ee-086b-4eed-889f-6dc3aa6fa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    type|\n",
      "+--------+\n",
      "|TRANSFER|\n",
      "| CASH_IN|\n",
      "|CASH_OUT|\n",
      "| PAYMENT|\n",
      "|   DEBIT|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018f7b95-fbf9-4319-a80f-fac331365873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|isFraud|  count|\n",
      "+-------+-------+\n",
      "|      1|   8213|\n",
      "|      0|6354407|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"isFraud\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e6a54-98e4-43d2-9559-b0a8273f8e96",
   "metadata": {},
   "source": [
    "we have a classic class imbalance problem:\n",
    "\n",
    "So only about 0.13% of your data is fraud, which means:\n",
    "\n",
    "A model that always predicts \"Not Fraud\" will still be 99.9% accurate â€” but completely useless! \n",
    "\n",
    "Weâ€™ll handle this later using class weighting or undersampling, but first, letâ€™s continue data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3746e45-309b-4a41-80bc-17e2a0b52060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+\n",
      "|    type|isFraud|  count|\n",
      "+--------+-------+-------+\n",
      "| CASH_IN|      0|1399284|\n",
      "|CASH_OUT|      0|2233384|\n",
      "|CASH_OUT|      1|   4116|\n",
      "|   DEBIT|      0|  41432|\n",
      "| PAYMENT|      0|2151495|\n",
      "|TRANSFER|      0| 528812|\n",
      "|TRANSFER|      1|   4097|\n",
      "+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"type\", \"isFraud\").count().orderBy(\"type\", \"isFraud\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96785275-a3e4-49cd-8da0-61945787046b",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "Fraud only happens in:\n",
    "\n",
    "- TRANSFER\n",
    "\n",
    "- CASH_OUT\n",
    "\n",
    "Other transaction types never have fraud (PAYMENT, CASH_IN, DEBIT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "757b97af-bb37-4143-818d-59e65ba1f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|            0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as Fsum\n",
    "\n",
    "# Sum of nulls in each column\n",
    "df.select([Fsum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e8709-ebb5-417e-a786-9ffe05c0a613",
   "metadata": {},
   "source": [
    "Now weâ€™ll keep only the rows where type is TRANSFER or CASH_OUT, because:\n",
    "\n",
    "- These are the only transaction types where fraud occurs\n",
    "\n",
    "- Including the others would add noise and confuse the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abfe4839-5723-49cb-ac4e-7534c300e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+\n",
      "|    type|isFraud|  count|\n",
      "+--------+-------+-------+\n",
      "|CASH_OUT|      0|2233384|\n",
      "|TRANSFER|      1|   4097|\n",
      "|CASH_OUT|      1|   4116|\n",
      "|TRANSFER|      0| 528812|\n",
      "+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter((df.type == \"TRANSFER\") | (df.type == \"CASH_OUT\"))\n",
    "df_filtered.select(\"type\", \"isFraud\").groupBy(\"type\", \"isFraud\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919dc3-4285-4d2f-9dba-e511fc668771",
   "metadata": {},
   "source": [
    "Some columns are not useful for machine learning because:\n",
    "\n",
    "- They contain IDs or names (nameOrig, nameDest) which wonâ€™t help the model\n",
    "- They may leak target info or be too specific\n",
    "- We only need useful numerical features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff4f925-27e1-4781-89c1-bcba83417f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
      "|    type|   amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|\n",
      "+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
      "|TRANSFER|    181.0|        181.0|           0.0|           0.0|           0.0|      1|\n",
      "|CASH_OUT|    181.0|        181.0|           0.0|       21182.0|           0.0|      1|\n",
      "|CASH_OUT|229133.94|      15325.0|           0.0|        5083.0|      51513.44|      0|\n",
      "|TRANSFER| 215310.3|        705.0|           0.0|       22425.0|           0.0|      0|\n",
      "|TRANSFER|311685.89|      10835.0|           0.0|        6267.0|    2719172.89|      0|\n",
      "+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "df_model = df_filtered.drop(\"nameOrig\", \"nameDest\", \"isFlaggedFraud\", \"step\")\n",
    "\n",
    "# View the first few rows\n",
    "df_model.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63d7b2-204a-460e-ba75-b1b1410ab38c",
   "metadata": {},
   "source": [
    "- Machine Learning models canâ€™t work with text labels like \"TRANSFER\" or \"CASH_OUT\" â€”\n",
    "We need to convert them into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4b2466-0769-4cef-8e03-2929b80bfd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|    type|type_index|\n",
      "+--------+----------+\n",
      "|CASH_OUT|       0.0|\n",
      "|TRANSFER|       1.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convert 'type' to numeric using StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "df_indexed = indexer.fit(df_model).transform(df_model)\n",
    "\n",
    "# Show the transformed result\n",
    "df_indexed.select(\"type\", \"type_index\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c74ce4-0ad3-469d-a70c-003f23051652",
   "metadata": {},
   "source": [
    "**Combine All Features into a Single Feature Vector**\n",
    "- MLlib models in PySpark expect one column named \"features\" that contains all input features packed together as a vector.\n",
    "\n",
    "- Weâ€™ll use *VectorAssembler* to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc0df19-2794-4f34-afdf-95dc608a55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------+\n",
      "|features                                     |isFraud|\n",
      "+---------------------------------------------+-------+\n",
      "|[181.0,181.0,0.0,0.0,0.0,1.0]                |1      |\n",
      "|[181.0,181.0,0.0,21182.0,0.0,0.0]            |1      |\n",
      "|[229133.94,15325.0,0.0,5083.0,51513.44,0.0]  |0      |\n",
      "|[215310.3,705.0,0.0,22425.0,0.0,1.0]         |0      |\n",
      "|[311685.89,10835.0,0.0,6267.0,2719172.89,1.0]|0      |\n",
      "+---------------------------------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_cols = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"]\n",
    "\n",
    "# Create the assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform the data\n",
    "df_final = assembler.transform(df_indexed)\n",
    "\n",
    "# Show the final DataFrame with features\n",
    "df_final.select(\"features\", \"isFraud\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f97dd-5a0d-4d5e-b528-5661ce2d1bdf",
   "metadata": {},
   "source": [
    "**Train a Classification Model (Logistic Regression)** :\n",
    " \n",
    " Weâ€™ll use *Logistic Regression* for now because:\n",
    "\n",
    "- Itâ€™s a standard model for binary classification\n",
    "- It works well as a baseline\n",
    "- It gives probabilities, which is helpful for fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82da80b-2c01-4988-8ec4-ac6282691758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------+----------+-------------------------------------------+\n",
      "|features                                     |isFraud|prediction|probability                                |\n",
      "+---------------------------------------------+-------+----------+-------------------------------------------+\n",
      "|[181.0,181.0,0.0,0.0,0.0,1.0]                |1      |0.0       |[0.9808727271585423,0.019127272841457654]  |\n",
      "|[181.0,181.0,0.0,21182.0,0.0,0.0]            |1      |0.0       |[0.9958119075800348,0.004188092419965206]  |\n",
      "|[229133.94,15325.0,0.0,5083.0,51513.44,0.0]  |0      |0.0       |[0.9999385873232427,6.141267675729978E-5]  |\n",
      "|[215310.3,705.0,0.0,22425.0,0.0,1.0]         |0      |0.0       |[0.9996283155591873,3.716844408127029E-4]  |\n",
      "|[311685.89,10835.0,0.0,6267.0,2719172.89,1.0]|0      |0.0       |[0.9999999999501343,4.9865667151038906E-11]|\n",
      "+---------------------------------------------+-------+----------+-------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"isFraud\")\n",
    "\n",
    "# Fit the model to the data\n",
    "lr_model = lr.fit(df_final)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(df_final)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ce58a2b-2b7c-4cc6-944a-db6e70842b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Area Under ROC Curve (AUC): 0.9820\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use the built-in evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluate using AUC-ROC (common for binary classification)\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"ðŸš€ Area Under ROC Curve (AUC): {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c849ae-bb9c-4421-a908-83ee104c47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš–ï¸ Class Weight (0 vs 1): 2762196 / 8213 = 336.32\n"
     ]
    }
   ],
   "source": [
    "# Count total fraud and non-fraud\n",
    "fraud_count = df_final.filter(df_final.isFraud == 1).count()\n",
    "nonfraud_count = df_final.filter(df_final.isFraud == 0).count()\n",
    "\n",
    "# Total rows\n",
    "total_count = fraud_count + nonfraud_count\n",
    "\n",
    "# Calculate balancing ratio\n",
    "balance_ratio = nonfraud_count / fraud_count\n",
    "print(f\"âš–ï¸ Class Weight (0 vs 1): {nonfraud_count} / {fraud_count} = {balance_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba9f12-26bf-4522-9bd9-ac31528e6461",
   "metadata": {},
   "source": [
    "This means:\n",
    "\n",
    "- Fraud transactions are 336 times rarer than normal ones â€” so we need to give fraud cases 336x more weight during training to help the model learn from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c9b5fe5-ce2b-4eea-b4b8-3ce38b90f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|isFraud|      classWeight|\n",
      "+-------+-----------------+\n",
      "|      1|336.3199805186899|\n",
      "|      0|              1.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Create new column 'classWeight' based on label\n",
    "df_weighted = df_final.withColumn(\n",
    "    \"classWeight\",\n",
    "    when(df_final.isFraud == 1, lit(balance_ratio)).otherwise(lit(1.0))\n",
    ")\n",
    "\n",
    "# Show example rows\n",
    "df_weighted.select(\"isFraud\", \"classWeight\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ffd035-742b-4669-91e5-4c0a82e9473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------------------------------+\n",
      "|isFraud|prediction|probability                                |\n",
      "+-------+----------+-------------------------------------------+\n",
      "|1      |1.0       |[0.3947840478627993,0.6052159521372007]    |\n",
      "|1      |0.0       |[0.7197118142070917,0.28028818579290826]   |\n",
      "|0      |0.0       |[0.8235703680978783,0.1764296319021217]    |\n",
      "|0      |1.0       |[0.3638526816945722,0.6361473183054278]    |\n",
      "|0      |0.0       |[0.9999999999999891,1.0880185641326534E-14]|\n",
      "+-------+----------+-------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a new logistic regression model using weight column\n",
    "lr_weighted = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"isFraud\",\n",
    "    weightCol=\"classWeight\"  # This is new!\n",
    ")\n",
    "\n",
    "# Train the model on the weighted dataset\n",
    "lr_weighted_model = lr_weighted.fit(df_weighted)\n",
    "\n",
    "# Make predictions\n",
    "predictions_weighted = lr_weighted_model.transform(df_weighted)\n",
    "\n",
    "# Show the first few predictions\n",
    "predictions_weighted.select(\"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82fd2b31-4614-497a-bb25-dc8622e9360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ New AUC with Class Weighting: 0.9756\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create evaluator for binary classification\n",
    "evaluator_weighted = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluate the weighted model\n",
    "auc_weighted = evaluator_weighted.evaluate(predictions_weighted)\n",
    "\n",
    "print(f\"ðŸš€ New AUC with Class Weighting: {auc_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986293c-a811-446f-8c64-e115b3dad2dd",
   "metadata": {},
   "source": [
    "Why this is good:\n",
    "The weighted model now predicts more actual frauds (recallâ†‘) even though AUC dropped a bit.\n",
    "\n",
    "ðŸ’¡ In real-world fraud detection:\n",
    "\n",
    "- Catching more fraud is more important than perfect AUC.\n",
    "\n",
    "- Even false positives are acceptable, but missing a fraud isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c8ce70a-4202-4838-a021-1b5d2182b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9348    0.9662   2762196\n",
      "           1     0.0388    0.8852    0.0744      8213\n",
      "\n",
      "    accuracy                         0.9347   2770409\n",
      "   macro avg     0.5192    0.9100    0.5203   2770409\n",
      "weighted avg     0.9968    0.9347    0.9635   2770409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark predictions to Pandas DataFrame\n",
    "preds_pd = predictions_weighted.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "# Import scikit-learn metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate and print classification report\n",
    "print(classification_report(preds_pd['isFraud'], preds_pd['prediction'], digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89141507-7f59-44db-b6aa-8124ed15bcb6",
   "metadata": {},
   "source": [
    "**observation**\n",
    "\n",
    "âœ… Recall = 88.52% for Fraud\n",
    "This is excellent.\n",
    "\n",
    "You are catching almost 9 out of 10 fraud cases!\n",
    "\n",
    "That's thanks to the class weighting you applied â€” it worked perfectly.\n",
    "\n",
    "âš ï¸ Precision = 3.88% for Fraud\n",
    "This is very low â€” meaning:\n",
    "\n",
    "Most transactions flagged as fraud by the model are actually not fraud.\n",
    "For every 100 fraud alerts, only ~4 are real fraud.\n",
    "\n",
    "ðŸ§  This is common in fraud detection â€” and often acceptable â€” because:\n",
    "\n",
    "Catching fraud is top priority âœ…\n",
    "\n",
    "False alerts can be manually reviewed or auto-flagged in production\n",
    "\n",
    "ðŸ¤ F1-Score = 7.44% for Fraud\n",
    "A balanced metric that reflects the trade-off:\n",
    "\n",
    "Precision is low\n",
    "\n",
    "Recall is high\n",
    "\n",
    "Improving F1-score is your next goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "964b4b16-9463-449a-bc52-871fb01f2941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 2216340\n",
      "Testing set count : 554069\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and testing (20%)\n",
    "train_data, test_data = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Show how many records in each set\n",
    "print(f\"Training set count: {train_data.count()}\")\n",
    "print(f\"Testing set count : {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b570e652-3b57-4f16-8a60-49c12450306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------------------+\n",
      "|isFraud|prediction|probability                             |\n",
      "+-------+----------+----------------------------------------+\n",
      "|0      |0.0       |[0.8122403802484877,0.18775961975151234]|\n",
      "|0      |0.0       |[0.7887736886997534,0.2112263113002466] |\n",
      "|0      |0.0       |[0.8066613595422412,0.19333864045775884]|\n",
      "|0      |0.0       |[0.7711090960153837,0.2288909039846163] |\n",
      "|0      |0.0       |[0.9598815611459302,0.04011843885406985]|\n",
      "+-------+----------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create logistic regression model with class weighting\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"isFraud\",\n",
    "    weightCol=\"classWeight\"  # still using our weight column\n",
    ")\n",
    "\n",
    "# Train the model on training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show example predictions\n",
    "test_predictions.select(\"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ddc199a-413c-4b06-9623-03f98037a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ AUC on Test Data: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# AUC evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator_test = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc_test = evaluator_test.evaluate(test_predictions)\n",
    "print(f\"ðŸš€ AUC on Test Data: {auc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0190e907-6ffe-48d4-be6e-fb7e2aad8857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9346    0.9660    552433\n",
      "           1     0.0387    0.8906    0.0742      1636\n",
      "\n",
      "    accuracy                         0.9344    554069\n",
      "   macro avg     0.5192    0.9126    0.5201    554069\n",
      "weighted avg     0.9968    0.9344    0.9634    554069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas for sklearn metrics\n",
    "test_preds_pd = test_predictions.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(test_preds_pd['isFraud'], test_preds_pd['prediction'], digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d310640-3c2d-441e-a36c-5a679fae2f11",
   "metadata": {},
   "source": [
    "### Train a Random Forest Classifier (With Class Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2dc44b5-c7bf-4f64-98a9-33bb8c87c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------------------+\n",
      "|isFraud|prediction|probability                             |\n",
      "+-------+----------+----------------------------------------+\n",
      "|0      |0.0       |[0.8780176652799115,0.1219823347200885] |\n",
      "|0      |0.0       |[0.8915183833835644,0.10848161661643556]|\n",
      "|0      |0.0       |[0.8780176652799115,0.1219823347200885] |\n",
      "|0      |0.0       |[0.934160468095669,0.0658395319043311]  |\n",
      "|0      |0.0       |[0.7301419271947941,0.2698580728052059] |\n",
      "+-------+----------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create Random Forest model with class weights\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"isFraud\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeight\",  # still using classWeight column\n",
    "    numTrees=50,              # number of decision trees\n",
    "    maxDepth=10               # maximum depth of trees\n",
    ")\n",
    "\n",
    "# Train on training data\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Predict on test data\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Show example predictions\n",
    "rf_predictions.select(\"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "176d5512-c65c-4ca6-a7cb-d5d73c31bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² AUC with Random Forest: 0.9967\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create evaluator for Random Forest\n",
    "evaluator_rf = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluate AUC\n",
    "auc_rf = evaluator_rf.evaluate(rf_predictions)\n",
    "print(f\"ðŸŒ² AUC with Random Forest: {auc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b550dff-cc7e-4892-be86-b40230463715",
   "metadata": {},
   "source": [
    "**Observation** : This is near perfect!\n",
    "\n",
    "- The model very effectively separates fraud from non-fraud\n",
    "\n",
    "- In real-world fraud detection, this is an excellent AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7af35ea0-b7d3-49a6-b643-1489ef22b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9809    0.9904    552433\n",
      "           1     0.1335    0.9933    0.2354      1636\n",
      "\n",
      "    accuracy                         0.9809    554069\n",
      "   macro avg     0.5667    0.9871    0.6129    554069\n",
      "weighted avg     0.9974    0.9809    0.9881    554069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas\n",
    "rf_preds_pd = rf_predictions.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate report\n",
    "print(classification_report(rf_preds_pd['isFraud'], rf_preds_pd['prediction'], digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9f334-cdaa-413c-9578-4aeede52f276",
   "metadata": {},
   "source": [
    "ðŸ” **Class-wise Interpretation**\n",
    "\n",
    "âœ… Class 0 (Non-Fraud)\n",
    "\n",
    "- Precision = 100% â†’ Almost no false positives flagged as fraud\n",
    "- Recall = 98.09% â†’ Almost all legit transactions are correctly classified\n",
    "\n",
    "âš ï¸ Class 1 (Fraud)\n",
    "\n",
    "- Precision = 13.35% â†’ Slightly better than Logistic Regression (was 3.87%)\n",
    "- Recall = 99.33% â†’ Incredible! Almost every fraud was caught\n",
    "- F1-Score = 23.54% â†’ Huge jump from logistic regression (was ~7.4%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af682f6b-4a11-475c-8cb5-d391061bc1f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a774534-febb-4fe7-b74d-f510d3f19b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transaction for prediction\n",
    "new_data = spark.createDataFrame([\n",
    "    (10000.0, 10000.0, 0.0, 0.0, 0.0, 1.0)  # values: amount, oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest, type_index\n",
    "], [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13d6e121-5653-41e0-aa56-cef5dd394d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"], outputCol=\"features\")\n",
    "new_data = assembler.transform(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebded6a3-a081-4bdd-9642-cc51264f7ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2091.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 302.0 failed 1 times, most recent failure: Lost task 0.0 in stage 302.0 (TID 974) (DESKTOP-C0TRA0G executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mtransform(new_data)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2091.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 302.0 failed 1 times, most recent failure: Lost task 0.0 in stage 302.0 (TID 974) (DESKTOP-C0TRA0G executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n"
     ]
    }
   ],
   "source": [
    "prediction = rf_model.transform(new_data)\n",
    "prediction.select(\"prediction\", \"probability\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2d07520-75f8-44a5-abb2-73fb5f10fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Importance\n",
      "1   oldbalanceOrg    0.502758\n",
      "4  newbalanceDest    0.156525\n",
      "0          amount    0.147381\n",
      "2  newbalanceOrig    0.097380\n",
      "3  oldbalanceDest    0.064319\n",
      "5      type_index    0.031637\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define your feature names (order matters!)\n",
    "features = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"]\n",
    "# Get importance\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "# Combine and sort\n",
    "fi_df = pd.DataFrame(list(zip(features, importances)), columns=[\"Feature\", \"Importance\"])\n",
    "fi_df.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "print(fi_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cd3a19f-de08-41c2-8c2e-e89b0f0feef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions.select(\"isFraud\", \"prediction\", \"probability\") \\\n",
    "              .toPandas() \\\n",
    "              .to_csv(\"rf_predictions_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa242f8-2e24-4ef0-9c07-4f5d24175854",
   "metadata": {},
   "source": [
    "ðŸ“Œ What it does:\n",
    "\n",
    "Converts the Spark DataFrame (rf_predictions) into a Pandas DataFrame.\n",
    "\n",
    "Saves it to your local directory as rf_predictions_output.csv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7f07c-34ba-45d5-9fd2-47d764130579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
