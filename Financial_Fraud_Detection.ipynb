{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa13814-00aa-4e4c-8990-f8cb83867418",
   "metadata": {},
   "source": [
    "**PySpark** is the Python API for Apache Spark.\n",
    "\n",
    "- Apache Spark is a powerful open-source engine for big data processing, real-time stream processing, and machine learning at scale.\n",
    "\n",
    "- PySpark lets you use Python to work with Spark‚Äôs capabilities, so you can write Python code to run fast parallel processing jobs across large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5550e13-3734-45f8-9b3b-f0727d71bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c6193-21a3-4d60-9a34-34c06fcae16a",
   "metadata": {},
   "source": [
    "##### from pyspark.sql import SparkSession\n",
    "This means:\n",
    "\n",
    "\"Import\" the SparkSession class from the pyspark.sql module.\n",
    "\n",
    "- SparkSession is the main entry point to work with DataFrames in PySpark.\n",
    "\n",
    "- Think of SparkSession as the control center where you start, configure, and run Spark commands in Python.\n",
    "\n",
    "- Import the SparkSession tool. Then, set up a Spark engine named ‚ÄòFinancial Fraud Detection‚Äô. If it's already running, use that. Otherwise, create a new one.\n",
    "- \n",
    "- Without SparkSession, you can't use PySpark features like read.csv(), DataFrame, SQL, or MLlib models.\n",
    "\n",
    "It's the first thing you write in any PySpark program ‚Äî like import pandas in pandas-based code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf22923-7675-4c0d-b81f-904595978d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create or get Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Financial Fraud Detection\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd931ed7-1f3f-4b2e-869f-089ea9910a39",
   "metadata": {},
   "source": [
    "üîπ **header=True**\n",
    "This tells Spark: ‚ÄúThe first row in the file contains the column names.‚Äù\n",
    "\n",
    "Without this, Spark would treat them as regular data.\n",
    "\n",
    "üîπ **inferSchema=True**\n",
    "This tells Spark: ‚ÄúTry to guess the correct data types for each column (like int, float, string).‚Äù\n",
    "\n",
    "Without this, Spark would treat all columns as strings, which is not what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497c76c2-23de-46ee-89a2-8017ccb7e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"fraudTrain.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dc99a6a-1a50-4dac-9df3-913d11a91b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c98124a-44b6-415d-a8df-7284f22d0739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83b0d3ee-086b-4eed-889f-6dc3aa6fa0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    type|\n",
      "+--------+\n",
      "|TRANSFER|\n",
      "| CASH_IN|\n",
      "|CASH_OUT|\n",
      "| PAYMENT|\n",
      "|   DEBIT|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "018f7b95-fbf9-4319-a80f-fac331365873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|isFraud|  count|\n",
      "+-------+-------+\n",
      "|      1|   8213|\n",
      "|      0|6354407|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"isFraud\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e6a54-98e4-43d2-9559-b0a8273f8e96",
   "metadata": {},
   "source": [
    "we have a classic class imbalance problem:\n",
    "\n",
    "So only about 0.13% of your data is fraud, which means:\n",
    "\n",
    "A model that always predicts \"Not Fraud\" will still be 99.9% accurate ‚Äî but completely useless! \n",
    "\n",
    "We‚Äôll handle this later using class weighting or undersampling, but first, let‚Äôs continue data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3746e45-309b-4a41-80bc-17e2a0b52060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+\n",
      "|    type|isFraud|  count|\n",
      "+--------+-------+-------+\n",
      "| CASH_IN|      0|1399284|\n",
      "|CASH_OUT|      0|2233384|\n",
      "|CASH_OUT|      1|   4116|\n",
      "|   DEBIT|      0|  41432|\n",
      "| PAYMENT|      0|2151495|\n",
      "|TRANSFER|      0| 528812|\n",
      "|TRANSFER|      1|   4097|\n",
      "+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"type\", \"isFraud\").count().orderBy(\"type\", \"isFraud\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96785275-a3e4-49cd-8da0-61945787046b",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "Fraud only happens in:\n",
    "\n",
    "- TRANSFER\n",
    "\n",
    "- CASH_OUT\n",
    "\n",
    "Other transaction types never have fraud (PAYMENT, CASH_IN, DEBIT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "757b97af-bb37-4143-818d-59e65ba1f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|step|type|amount|nameOrig|oldbalanceOrg|newbalanceOrig|nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "|   0|   0|     0|       0|            0|             0|       0|             0|             0|      0|             0|\n",
      "+----+----+------+--------+-------------+--------------+--------+--------------+--------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as Fsum\n",
    "\n",
    "# Sum of nulls in each column\n",
    "df.select([Fsum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e8709-ebb5-417e-a786-9ffe05c0a613",
   "metadata": {},
   "source": [
    "Now we‚Äôll keep only the rows where type is TRANSFER or CASH_OUT, because:\n",
    "\n",
    "- These are the only transaction types where fraud occurs\n",
    "\n",
    "- Including the others would add noise and confuse the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abfe4839-5723-49cb-ac4e-7534c300e05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+\n",
      "|    type|isFraud|  count|\n",
      "+--------+-------+-------+\n",
      "|CASH_OUT|      0|2233384|\n",
      "|TRANSFER|      1|   4097|\n",
      "|CASH_OUT|      1|   4116|\n",
      "|TRANSFER|      0| 528812|\n",
      "+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter((df.type == \"TRANSFER\") | (df.type == \"CASH_OUT\"))\n",
    "df_filtered.select(\"type\", \"isFraud\").groupBy(\"type\", \"isFraud\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919dc3-4285-4d2f-9dba-e511fc668771",
   "metadata": {},
   "source": [
    "Some columns are not useful for machine learning because:\n",
    "\n",
    "- They contain IDs or names (nameOrig, nameDest) which won‚Äôt help the model\n",
    "- They may leak target info or be too specific\n",
    "- We only need useful numerical features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ff4f925-27e1-4781-89c1-bcba83417f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
      "|    type|   amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|\n",
      "+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
      "|TRANSFER|    181.0|        181.0|           0.0|           0.0|           0.0|      1|\n",
      "|CASH_OUT|    181.0|        181.0|           0.0|       21182.0|           0.0|      1|\n",
      "|CASH_OUT|229133.94|      15325.0|           0.0|        5083.0|      51513.44|      0|\n",
      "|TRANSFER| 215310.3|        705.0|           0.0|       22425.0|           0.0|      0|\n",
      "|TRANSFER|311685.89|      10835.0|           0.0|        6267.0|    2719172.89|      0|\n",
      "+--------+---------+-------------+--------------+--------------+--------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "df_model = df_filtered.drop(\"nameOrig\", \"nameDest\", \"isFlaggedFraud\", \"step\")\n",
    "\n",
    "# View the first few rows\n",
    "df_model.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63d7b2-204a-460e-ba75-b1b1410ab38c",
   "metadata": {},
   "source": [
    "- Machine Learning models can‚Äôt work with text labels like \"TRANSFER\" or \"CASH_OUT\" ‚Äî\n",
    "We need to convert them into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4b2466-0769-4cef-8e03-2929b80bfd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|    type|type_index|\n",
      "+--------+----------+\n",
      "|CASH_OUT|       0.0|\n",
      "|TRANSFER|       1.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convert 'type' to numeric using StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"type\", outputCol=\"type_index\")\n",
    "df_indexed = indexer.fit(df_model).transform(df_model)\n",
    "\n",
    "# Show the transformed result\n",
    "df_indexed.select(\"type\", \"type_index\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c74ce4-0ad3-469d-a70c-003f23051652",
   "metadata": {},
   "source": [
    "**Combine All Features into a Single Feature Vector**\n",
    "- MLlib models in PySpark expect one column named \"features\" that contains all input features packed together as a vector.\n",
    "\n",
    "- We‚Äôll use *VectorAssembler* to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bc0df19-2794-4f34-afdf-95dc608a55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------+\n",
      "|features                                     |isFraud|\n",
      "+---------------------------------------------+-------+\n",
      "|[181.0,181.0,0.0,0.0,0.0,1.0]                |1      |\n",
      "|[181.0,181.0,0.0,21182.0,0.0,0.0]            |1      |\n",
      "|[229133.94,15325.0,0.0,5083.0,51513.44,0.0]  |0      |\n",
      "|[215310.3,705.0,0.0,22425.0,0.0,1.0]         |0      |\n",
      "|[311685.89,10835.0,0.0,6267.0,2719172.89,1.0]|0      |\n",
      "+---------------------------------------------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define the feature columns\n",
    "feature_cols = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"]\n",
    "\n",
    "# Create the assembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform the data\n",
    "df_final = assembler.transform(df_indexed)\n",
    "\n",
    "# Show the final DataFrame with features\n",
    "df_final.select(\"features\", \"isFraud\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f97dd-5a0d-4d5e-b528-5661ce2d1bdf",
   "metadata": {},
   "source": [
    "**Train a Classification Model (Logistic Regression)** :\n",
    " \n",
    " We‚Äôll use *Logistic Regression* for now because:\n",
    "\n",
    "- It‚Äôs a standard model for binary classification\n",
    "- It works well as a baseline\n",
    "- It gives probabilities, which is helpful for fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a82da80b-2c01-4988-8ec4-ac6282691758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-------+----------+-------------------------------------------+\n",
      "|features                                     |isFraud|prediction|probability                                |\n",
      "+---------------------------------------------+-------+----------+-------------------------------------------+\n",
      "|[181.0,181.0,0.0,0.0,0.0,1.0]                |1      |0.0       |[0.9808727271585423,0.019127272841457654]  |\n",
      "|[181.0,181.0,0.0,21182.0,0.0,0.0]            |1      |0.0       |[0.9958119075800348,0.004188092419965206]  |\n",
      "|[229133.94,15325.0,0.0,5083.0,51513.44,0.0]  |0      |0.0       |[0.9999385873232427,6.141267675729978E-5]  |\n",
      "|[215310.3,705.0,0.0,22425.0,0.0,1.0]         |0      |0.0       |[0.9996283155591873,3.716844408127029E-4]  |\n",
      "|[311685.89,10835.0,0.0,6267.0,2719172.89,1.0]|0      |0.0       |[0.9999999999501343,4.9865667151038906E-11]|\n",
      "+---------------------------------------------+-------+----------+-------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"isFraud\")\n",
    "\n",
    "# Fit the model to the data\n",
    "lr_model = lr.fit(df_final)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(df_final)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ce58a2b-2b7c-4cc6-944a-db6e70842b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Area Under ROC Curve (AUC): 0.9820\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Use the built-in evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluate using AUC-ROC (common for binary classification)\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"üöÄ Area Under ROC Curve (AUC): {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c849ae-bb9c-4421-a908-83ee104c47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Class Weight (0 vs 1): 2762196 / 8213 = 336.32\n"
     ]
    }
   ],
   "source": [
    "# Count total fraud and non-fraud\n",
    "fraud_count = df_final.filter(df_final.isFraud == 1).count()\n",
    "nonfraud_count = df_final.filter(df_final.isFraud == 0).count()\n",
    "\n",
    "# Total rows\n",
    "total_count = fraud_count + nonfraud_count\n",
    "\n",
    "# Calculate balancing ratio\n",
    "balance_ratio = nonfraud_count / fraud_count\n",
    "print(f\"‚öñÔ∏è Class Weight (0 vs 1): {nonfraud_count} / {fraud_count} = {balance_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba9f12-26bf-4522-9bd9-ac31528e6461",
   "metadata": {},
   "source": [
    "This means:\n",
    "\n",
    "- Fraud transactions are 336 times rarer than normal ones ‚Äî so we need to give fraud cases 336x more weight during training to help the model learn from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c9b5fe5-ce2b-4eea-b4b8-3ce38b90f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|isFraud|      classWeight|\n",
      "+-------+-----------------+\n",
      "|      1|336.3199805186899|\n",
      "|      0|              1.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Create new column 'classWeight' based on label\n",
    "df_weighted = df_final.withColumn(\n",
    "    \"classWeight\",\n",
    "    when(df_final.isFraud == 1, lit(balance_ratio)).otherwise(lit(1.0))\n",
    ")\n",
    "\n",
    "# Show example rows\n",
    "df_weighted.select(\"isFraud\", \"classWeight\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ffd035-742b-4669-91e5-4c0a82e9473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------------------------------------+\n",
      "|isFraud|prediction|probability                                |\n",
      "+-------+----------+-------------------------------------------+\n",
      "|1      |1.0       |[0.3947840478627993,0.6052159521372007]    |\n",
      "|1      |0.0       |[0.7197118142070917,0.28028818579290826]   |\n",
      "|0      |0.0       |[0.8235703680978783,0.1764296319021217]    |\n",
      "|0      |1.0       |[0.3638526816945722,0.6361473183054278]    |\n",
      "|0      |0.0       |[0.9999999999999891,1.0880185641326534E-14]|\n",
      "+-------+----------+-------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a new logistic regression model using weight column\n",
    "lr_weighted = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"isFraud\",\n",
    "    weightCol=\"classWeight\"  # This is new!\n",
    ")\n",
    "\n",
    "# Train the model on the weighted dataset\n",
    "lr_weighted_model = lr_weighted.fit(df_weighted)\n",
    "\n",
    "# Make predictions\n",
    "predictions_weighted = lr_weighted_model.transform(df_weighted)\n",
    "\n",
    "# Show the first few predictions\n",
    "predictions_weighted.select(\"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82fd2b31-4614-497a-bb25-dc8622e9360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ New AUC with Class Weighting: 0.9756\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create evaluator for binary classification\n",
    "evaluator_weighted = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluate the weighted model\n",
    "auc_weighted = evaluator_weighted.evaluate(predictions_weighted)\n",
    "\n",
    "print(f\"üöÄ New AUC with Class Weighting: {auc_weighted:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986293c-a811-446f-8c64-e115b3dad2dd",
   "metadata": {},
   "source": [
    "Why this is good:\n",
    "The weighted model now predicts more actual frauds (recall‚Üë) even though AUC dropped a bit.\n",
    "\n",
    "üí° In real-world fraud detection:\n",
    "\n",
    "- Catching more fraud is more important than perfect AUC.\n",
    "\n",
    "- Even false positives are acceptable, but missing a fraud isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c8ce70a-4202-4838-a021-1b5d2182b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9996    0.9348    0.9662   2762196\n",
      "           1     0.0388    0.8852    0.0744      8213\n",
      "\n",
      "    accuracy                         0.9347   2770409\n",
      "   macro avg     0.5192    0.9100    0.5203   2770409\n",
      "weighted avg     0.9968    0.9347    0.9635   2770409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Spark predictions to Pandas DataFrame\n",
    "preds_pd = predictions_weighted.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "# Import scikit-learn metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate and print classification report\n",
    "print(classification_report(preds_pd['isFraud'], preds_pd['prediction'], digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89141507-7f59-44db-b6aa-8124ed15bcb6",
   "metadata": {},
   "source": [
    "**observation**\n",
    "\n",
    "‚úÖ Recall = 88.52% for Fraud\n",
    "This is excellent.\n",
    "\n",
    "You are catching almost 9 out of 10 fraud cases!\n",
    "\n",
    "That's thanks to the class weighting you applied ‚Äî it worked perfectly.\n",
    "\n",
    "‚ö†Ô∏è Precision = 3.88% for Fraud\n",
    "This is very low ‚Äî meaning:\n",
    "\n",
    "Most transactions flagged as fraud by the model are actually not fraud.\n",
    "For every 100 fraud alerts, only ~4 are real fraud.\n",
    "\n",
    "üß† This is common in fraud detection ‚Äî and often acceptable ‚Äî because:\n",
    "\n",
    "Catching fraud is top priority ‚úÖ\n",
    "\n",
    "False alerts can be manually reviewed or auto-flagged in production\n",
    "\n",
    "ü§ù F1-Score = 7.44% for Fraud\n",
    "A balanced metric that reflects the trade-off:\n",
    "\n",
    "Precision is low\n",
    "\n",
    "Recall is high\n",
    "\n",
    "Improving F1-score is your next goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "964b4b16-9463-449a-bc52-871fb01f2941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 2216340\n",
      "Testing set count : 554069\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training (80%) and testing (20%)\n",
    "train_data, test_data = df_weighted.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Show how many records in each set\n",
    "print(f\"Training set count: {train_data.count()}\")\n",
    "print(f\"Testing set count : {test_data.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b570e652-3b57-4f16-8a60-49c12450306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------------------+\n",
      "|isFraud|prediction|probability                             |\n",
      "+-------+----------+----------------------------------------+\n",
      "|0      |0.0       |[0.8122403802484877,0.18775961975151234]|\n",
      "|0      |0.0       |[0.7887736886997534,0.2112263113002466] |\n",
      "|0      |0.0       |[0.8066613595422412,0.19333864045775884]|\n",
      "|0      |0.0       |[0.7711090960153837,0.2288909039846163] |\n",
      "|0      |0.0       |[0.9598815611459302,0.04011843885406985]|\n",
      "+-------+----------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create logistic regression model with class weighting\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"isFraud\",\n",
    "    weightCol=\"classWeight\"  # still using our weight column\n",
    ")\n",
    "\n",
    "# Train the model on training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show example predictions\n",
    "test_predictions.select(\"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ddc199a-413c-4b06-9623-03f98037a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ AUC on Test Data: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# AUC evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator_test = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc_test = evaluator_test.evaluate(test_predictions)\n",
    "print(f\"üöÄ AUC on Test Data: {auc_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0190e907-6ffe-48d4-be6e-fb7e2aad8857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9997    0.9346    0.9660    552433\n",
      "           1     0.0387    0.8906    0.0742      1636\n",
      "\n",
      "    accuracy                         0.9344    554069\n",
      "   macro avg     0.5192    0.9126    0.5201    554069\n",
      "weighted avg     0.9968    0.9344    0.9634    554069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas for sklearn metrics\n",
    "test_preds_pd = test_predictions.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(test_preds_pd['isFraud'], test_preds_pd['prediction'], digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d310640-3c2d-441e-a36c-5a679fae2f11",
   "metadata": {},
   "source": [
    "### Train a Random Forest Classifier (With Class Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2dc44b5-c7bf-4f64-98a9-33bb8c87c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------------------------------------+\n",
      "|isFraud|prediction|probability                             |\n",
      "+-------+----------+----------------------------------------+\n",
      "|0      |0.0       |[0.8780176652799115,0.1219823347200885] |\n",
      "|0      |0.0       |[0.8915183833835644,0.10848161661643556]|\n",
      "|0      |0.0       |[0.8780176652799115,0.1219823347200885] |\n",
      "|0      |0.0       |[0.934160468095669,0.0658395319043311]  |\n",
      "|0      |0.0       |[0.7301419271947941,0.2698580728052059] |\n",
      "+-------+----------+----------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create Random Forest model with class weights\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"isFraud\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeight\",  # still using classWeight column\n",
    "    numTrees=50,              # number of decision trees\n",
    "    maxDepth=10               # maximum depth of trees\n",
    ")\n",
    "\n",
    "# Train on training data\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Predict on test data\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Show example predictions\n",
    "rf_predictions.select(\"isFraud\", \"prediction\", \"probability\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "176d5512-c65c-4ca6-a7cb-d5d73c31bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ AUC with Random Forest: 0.9967\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create evaluator for Random Forest\n",
    "evaluator_rf = BinaryClassificationEvaluator(\n",
    "    labelCol=\"isFraud\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Evaluate AUC\n",
    "auc_rf = evaluator_rf.evaluate(rf_predictions)\n",
    "print(f\"üå≤ AUC with Random Forest: {auc_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b550dff-cc7e-4892-be86-b40230463715",
   "metadata": {},
   "source": [
    "**Observation** : This is near perfect!\n",
    "\n",
    "- The model very effectively separates fraud from non-fraud\n",
    "\n",
    "- In real-world fraud detection, this is an excellent AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7af35ea0-b7d3-49a6-b643-1489ef22b63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9809    0.9904    552433\n",
      "           1     0.1335    0.9933    0.2354      1636\n",
      "\n",
      "    accuracy                         0.9809    554069\n",
      "   macro avg     0.5667    0.9871    0.6129    554069\n",
      "weighted avg     0.9974    0.9809    0.9881    554069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas\n",
    "rf_preds_pd = rf_predictions.select(\"isFraud\", \"prediction\").toPandas()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate report\n",
    "print(classification_report(rf_preds_pd['isFraud'], rf_preds_pd['prediction'], digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9f334-cdaa-413c-9578-4aeede52f276",
   "metadata": {},
   "source": [
    "üîç **Class-wise Interpretation**\n",
    "\n",
    "‚úÖ Class 0 (Non-Fraud)\n",
    "\n",
    "- Precision = 100% ‚Üí Almost no false positives flagged as fraud\n",
    "- Recall = 98.09% ‚Üí Almost all legit transactions are correctly classified\n",
    "\n",
    "‚ö†Ô∏è Class 1 (Fraud)\n",
    "\n",
    "- Precision = 13.35% ‚Üí Slightly better than Logistic Regression (was 3.87%)\n",
    "- Recall = 99.33% ‚Üí Incredible! Almost every fraud was caught\n",
    "- F1-Score = 23.54% ‚Üí Huge jump from logistic regression (was ~7.4%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af682f6b-4a11-475c-8cb5-d391061bc1f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a774534-febb-4fe7-b74d-f510d3f19b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample transaction for prediction\n",
    "new_data = spark.createDataFrame([\n",
    "    (10000.0, 10000.0, 0.0, 0.0, 0.0, 1.0)  # values: amount, oldbalanceOrg, newbalanceOrig, oldbalanceDest, newbalanceDest, type_index\n",
    "], [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13d6e121-5653-41e0-aa56-cef5dd394d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"], outputCol=\"features\")\n",
    "new_data = assembler.transform(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebded6a3-a081-4bdd-9642-cc51264f7ce7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2091.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 302.0 failed 1 times, most recent failure: Lost task 0.0 in stage 302.0 (TID 974) (DESKTOP-C0TRA0G executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mtransform(new_data)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2091.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 302.0 failed 1 times, most recent failure: Lost task 0.0 in stage 302.0 (TID 974) (DESKTOP-C0TRA0G executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:252)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:143)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:178)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:261)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Timed out while waiting for the Python worker to connect back\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:234)\r\n\t... 33 more\r\n"
     ]
    }
   ],
   "source": [
    "prediction = rf_model.transform(new_data)\n",
    "prediction.select(\"prediction\", \"probability\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2d07520-75f8-44a5-abb2-73fb5f10fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Importance\n",
      "1   oldbalanceOrg    0.502758\n",
      "4  newbalanceDest    0.156525\n",
      "0          amount    0.147381\n",
      "2  newbalanceOrig    0.097380\n",
      "3  oldbalanceDest    0.064319\n",
      "5      type_index    0.031637\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define your feature names (order matters!)\n",
    "features = [\"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"type_index\"]\n",
    "# Get importance\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "# Combine and sort\n",
    "fi_df = pd.DataFrame(list(zip(features, importances)), columns=[\"Feature\", \"Importance\"])\n",
    "fi_df.sort_values(by=\"Importance\", ascending=False, inplace=True)\n",
    "print(fi_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cd3a19f-de08-41c2-8c2e-e89b0f0feef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions.select(\"isFraud\", \"prediction\", \"probability\") \\\n",
    "              .toPandas() \\\n",
    "              .to_csv(\"rf_predictions_output.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa242f8-2e24-4ef0-9c07-4f5d24175854",
   "metadata": {},
   "source": [
    "üìå What it does:\n",
    "\n",
    "Converts the Spark DataFrame (rf_predictions) into a Pandas DataFrame.\n",
    "\n",
    "Saves it to your local directory as rf_predictions_output.csv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7f07c-34ba-45d5-9fd2-47d764130579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
